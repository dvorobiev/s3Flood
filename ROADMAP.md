## Roadmap

### Высокий приоритет
- [ ] **Автотесты: pytest + smoke-сценарий против MinIO в CI (Linux и Windows).**
  
  **Что это:** Автоматизированные тесты для проверки работоспособности s3flood перед каждым коммитом/релизом.
  
  **Как планируем реализовать:**
  - Настроить GitHub Actions workflow для Linux и Windows runners
  - Запускать MinIO в Docker контейнере в CI
  - Написать pytest-тесты:
    - Smoke-тест: создать минимальный датасет (например, 10MB), запустить профиль `write` с 2 потоками, проверить что файлы загрузились в бакет
    - Проверка метрик: убедиться что CSV и JSON отчёты генерируются корректно
    - Проверка профилей: базовый прогон `read` и `mixed` профилей
  - Интеграция в CI pipeline: тесты запускаются автоматически на каждый PR и push в main
  - Цель: предотвратить регрессии и обеспечить стабильность базовой функциональности
- [x] Профили нагрузки: `read-heavy`, `write-heavy`, `mixed` (ранее `mixed-70-30`), паттерны `sustained|bursty` — реализовано.
- [x] Тест чтения из бакета (поток в `/dev/null`, без нагрузки на диск) — реализовано в профиле `write-heavy`.
- [x] Смешанные режимы (одновременный upload/download, общая очередь) — реализовано в профиле `mixed`.
- [ ] Удаление загруженных объектов после завершения профиля или по опции `--cleanup`.
- [x] Ретраи с экспоненциальным backoff, таймауты, лимиты очередей — реализовано.
- [x] Кластерный режим: чтение через тот же endpoint, через который был записан объект — реализовано.

### Средний приоритет
- [ ] **Интерфейс `Runner` и дополнительные клиенты: `s5cmd`, `rclone`, (в перспективе — кастомные агенты).**
  
  **Что это:** Абстракция над S3-клиентами, позволяющая использовать не только AWS CLI, но и альтернативные инструменты для операций с S3.
  
  **Как планируем реализовать:**
  - Создать абстрактный интерфейс `Runner` (или `S3Client`) с методами: `upload()`, `download()`, `list_objects()`, `delete_object()`
  - Реализовать конкретные классы:
    - `AWSCLIRunner` (текущая реализация через `aws s3 cp` / `aws s3api`)
    - `S5cmdRunner` (через `s5cmd cp` — быстрый параллельный клиент)
    - `RcloneRunner` (через `rclone copy` — универсальный инструмент)
  - Добавить в конфиг поле `client: awscli|s5cmd|rclone` (уже есть в схеме, но используется только `awscli`)
  - Адаптировать логику в `executor.py` для работы через абстракцию вместо прямых вызовов `subprocess.run(["aws", ...])`
  - Преимущества: возможность сравнения производительности разных клиентов, использование более быстрых инструментов (s5cmd), гибкость выбора инструмента под задачу
- [x] Расширенный CLI/TUI: меню выбора профиля, статуса и конфигов — реализовано в интерактивном меню (`--interactive`).
- [ ] **Расширенные метрики: P50/P90/P99 для всех операций, экспорт в Prometheus-совместимый формат.**
  
  **Что это:** Детальная статистика по процентилям латентности и скорости, а также интеграция с системами мониторинга.
  
  **Как планируем реализовать:**
  - **Процентили латентности:**
    - Расширить класс `Metrics` для расчёта P50/P90/P95/P99 по `latency_ms` для upload и download операций отдельно
    - Добавить расчёт процентилей по скорости (MB/s) для успешных операций
    - Выводить в дашборд и в итоговый JSON-отчёт (`report.json`)
  - **Prometheus-совместимый экспорт:**
    - Добавить опцию `--metrics-format prometheus` или `metrics_format: prometheus` в конфиг
    - Генерировать файл в формате Prometheus text-based exposition format (`.prom` файл)
    - Метрики: `s3flood_operations_total{op="upload|download",status="ok|err"}`, `s3flood_bytes_total{op="upload|download"}`, `s3flood_latency_seconds{op="upload|download",quantile="0.5|0.9|0.99"}`, `s3flood_speed_mbps{op="upload|download"}`
    - Возможность отправки метрик в Pushgateway или через HTTP endpoint для scraping
  - **Интеграция с Grafana:** пример дашборда для визуализации метрик из Prometheus
- [ ] Документация: примеры конфигов (`examples/configs/*.yaml`), подробные гайды по профилям, Quickstart Linux/Windows.
- [ ] **Поддержка чтения/записи с ограничениями IOPS/throughput, базовый throttling.**
  
  **Что это:** Возможность ограничить скорость операций (например, симулировать медленное соединение или тестировать поведение при ограниченной пропускной способности).
  
  **Как планируем реализовать:**
  - Добавить в конфиг параметры:
    - `max_throughput_mbps: float | None` — максимальная скорость в MB/s (общая для всех потоков)
    - `max_iops: int | None` — максимальное количество операций в секунду
    - `throttle_per_thread: bool` — применять лимиты на каждый поток отдельно или глобально
  - Реализовать throttling механизм:
    - Использовать `time.sleep()` для задержек между операциями или внутри операций (для throughput)
    - Для IOPS: счётчик операций в секунду, блокировка при превышении лимита
    - Для throughput: измерение скорости передачи данных, паузы для соблюдения лимита
  - Интеграция в `executor.py`: проверка лимитов перед/во время выполнения операций
  - Применение: симуляция реальных сетевых условий, тестирование устойчивости S3-бэкенда при ограниченной пропускной способности, нагрузочное тестирование с контролируемой интенсивностью
- [ ] Настройки параметров AWS CLI: размер чанка (multipart chunk size), multipart threshold, max concurrent requests и другие параметры производительности.

### Низкий приоритет / дальше
- [ ] PyInstaller/standalone сборки для Linux/Windows.
- [ ] **Расширенные распределения наборов данных (Pareto/Zipf).**
  
  **Что это:** Генерация датасетов с реалистичными распределениями размеров файлов, имитирующими реальные паттерны использования (большинство файлов маленькие, но есть несколько очень больших).
  
  **Как планируем реализовать:**
  - **Pareto распределение (80/20 правило):**
    - 80% файлов маленькие, 20% — большие
    - Параметры: `pareto_alpha` (степень неравномерности), `pareto_min_size`, `pareto_max_size`
    - Использование библиотеки `numpy.random.pareto()` для генерации размеров
  - **Zipf распределение:**
    - Ещё более реалистичное распределение для веб-контента и файловых систем
    - Параметры: `zipf_a` (параметр формы), диапазон размеров
    - Использование `numpy.random.zipf()` для генерации
  - **Интеграция в `dataset.py`:**
    - Добавить опцию `distribution: uniform|pareto|zipf` в параметры генерации датасета
    - При выборе `pareto`/`zipf` — генерировать размеры файлов по соответствующему распределению вместо равномерного
    - Сохранять метаданные о распределении в датасете для воспроизводимости
  - **Применение:** более реалистичные тесты, имитация реальных рабочих нагрузок (например, веб-сервер с большим количеством маленьких статических файлов и несколькими большими медиа-файлами)
- [x] Улучшенный TUI (rich/textual) с графиками в реальном времени — базовый TUI на rich/questionary реализован (графики в реальном времени — в планах).


